{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Installation\n",
        "\n"
      ],
      "metadata": {
        "id": "vGLMBeNg-CHh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TDRNq2wV9Yxr"
      },
      "outputs": [],
      "source": [
        "pip install wandb numpy pandas matplotlib torch torchvision"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset"
      ],
      "metadata": {
        "id": "2pKvVtLV-TqH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zftJw42q-LCj",
        "outputId": "5278f293-4ccc-4159-9e93-bfe67cf1ba47"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!unzip /content/drive/MyDrive/DL_Assignment2/Dataset/nature_12K.zip -d /content/drive/MyDrive/DL_Assignment2/Dataset"
      ],
      "metadata": {
        "id": "Vk6BRhrcAydF",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Libraries"
      ],
      "metadata": {
        "id": "4cSWu4KFCJLE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import os\n",
        "from torchvision import datasets, transforms\n",
        "from sklearn.model_selection import train_test_split, StratifiedShuffleSplit\n",
        "from torch.utils.data import Subset, DataLoader\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import wandb\n",
        "import tqdm\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ],
      "metadata": {
        "id": "6a0b6a18CMBf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f179747b-00b2-49e6-e5b0-0b4547cc7d4a"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset loader"
      ],
      "metadata": {
        "id": "lF5ZbaMoGFGm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def validationDataSplit(train_dataset):\n",
        "  classLabels = [label for _,label in train_dataset.samples]\n",
        "  num_classes = len(np.unique(classLabels))\n",
        "\n",
        "  sss = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
        "  train_indices, val_indices = next(sss.split(train_dataset.samples, classLabels))\n",
        "\n",
        "  train_subset = Subset(train_dataset, train_indices)\n",
        "  val_subset = Subset(train_dataset, val_indices)\n",
        "  return train_subset, val_subset, num_classes\n",
        "\n",
        "\n",
        "def load_data(base_dir, isDataAug):\n",
        "  train_dir = os.path.join(base_dir, 'train')\n",
        "  test_dir = os.path.join(base_dir, 'val')\n",
        "\n",
        "  train_transform, test_transform = None, None\n",
        "\n",
        "  if isDataAug == False:\n",
        "    train_transform = transforms.Compose([\n",
        "      transforms.Resize(256),\n",
        "      transforms.CenterCrop(224),\n",
        "      transforms.ToTensor(),\n",
        "      transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])\n",
        "  else:\n",
        "    train_transform = transforms.Compose([\n",
        "      transforms.Resize(256),\n",
        "      transforms.CenterCrop(224),\n",
        "      transforms.RandomHorizontalFlip(),\n",
        "      transforms.RandomRotation(10),\n",
        "      transforms.ToTensor(),\n",
        "      transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])\n",
        "\n",
        "  test_transform = transforms.Compose([\n",
        "        transforms.Resize(256),\n",
        "        transforms.CenterCrop(224),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])\n",
        "\n",
        "  train_dataset = datasets.ImageFolder(train_dir, transform=train_transform)\n",
        "  test_dataset = datasets.ImageFolder(test_dir, transform=test_transform)\n",
        "  train_dataset, val_dataset, num_classes = validationDataSplit(train_dataset)\n",
        "\n",
        "  # print(f\"inp: {train_dataset[0][0].shape} {train_dataset[0][1]}\")\n",
        "\n",
        "  train_loader = DataLoader(train_dataset,shuffle=True,num_workers=2,batch_size=64,pin_memory=True)\n",
        "  test_loader = DataLoader(test_dataset,shuffle=True,num_workers=2,batch_size=64,pin_memory=True)\n",
        "  val_loader = DataLoader(val_dataset,shuffle=True,num_workers=2,batch_size=64,pin_memory=True)\n",
        "\n",
        "  return train_loader, test_loader, val_loader, num_classes\n",
        "\n",
        "# load_data(\"/content/drive/MyDrive/DL_Assignment2/Dataset/inaturalist_12K/\", True)"
      ],
      "metadata": {
        "id": "17mDOMKWGIGJ"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Network"
      ],
      "metadata": {
        "id": "5T2fCf1l2ZJF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ConvolutionalNeuralNetwork(nn.Module):\n",
        "  activationFunctionsMap = {\"ReLU\": nn.ReLU, \"GELU\": nn.GELU, \"SiLU\": nn.SiLU}\n",
        "  # optimizersMap = {\"sgd\": optim.SGD, \"rmsprop\": optim.RMSprop, \"adam\": optim.Adam}\n",
        "\n",
        "  def __init__(self, num_classes,\n",
        "               num_filters, filter_sizes,\n",
        "               activationFun, optimizer,\n",
        "               n_neurons_denseLayer,\n",
        "               isBatchNormalization, dropout,\n",
        "               learning_rate=0.001,\n",
        "               momentum=0.5, beta = 0.9,\n",
        "               beta1=0.9, beta2=0.99,\n",
        "               epsilon=1e-8, weight_decay=0.0001):\n",
        "    super(ConvolutionalNeuralNetwork, self).__init__()\n",
        "    self.num_classes = num_classes\n",
        "    self.num_filters = num_filters\n",
        "    self.filter_sizes = filter_sizes\n",
        "    self.activationFun = ConvolutionalNeuralNetwork.activationFunctionsMap[activationFun]\n",
        "    # self.optimizer = ConvolutionalNeuralNetwork.optimizersMap[optimizer]\n",
        "\n",
        "    self.n_neurons_denseLayer = n_neurons_denseLayer\n",
        "    self.isBatchNormalization = isBatchNormalization\n",
        "    self.dropout = dropout\n",
        "\n",
        "    self.lr = learning_rate\n",
        "    self.momentum = momentum\n",
        "    self.betas = (beta1, beta2)\n",
        "    self.eps = epsilon\n",
        "    self.alpha = beta\n",
        "    self.weight_decay = weight_decay\n",
        "\n",
        "    self.defineModel()\n",
        "\n",
        "    if(optimizer == \"sgd\"):\n",
        "      self.optimizer = optim.SGD(self.parameters(), lr=self.lr, momentum=self.momentum, weight_decay=self.weight_decay)\n",
        "    elif(optimizer == \"rmsprop\"):\n",
        "      self.optimizer = optim.RMSprop(self.parameters(), lr=self.lr, alpha=self.alpha, eps=self.eps, weight_decay=self.weight_decay)\n",
        "    elif(optimizer == \"adam\"):\n",
        "      self.optimizer = optim.Adam(self.parameters(), lr=self.lr, betas=self.betas, eps=self.eps, weight_decay=self.weight_decay)\n",
        "\n",
        "\n",
        "\n",
        "  def defineModel(self):\n",
        "    self.model = nn.Sequential()\n",
        "\n",
        "    inChannels = 3;     # RGB channels for inaturalist\n",
        "    for i in range(len(self.num_filters)):\n",
        "      self.model.append(nn.Conv2d(inChannels, self.num_filters[i], self.filter_sizes[i], padding=self.filter_sizes[i]//2))\n",
        "      if self.isBatchNormalization:\n",
        "        self.model.append(nn.BatchNorm2d(self.num_filters[i]))\n",
        "      self.model.append(self.activationFun())\n",
        "      self.model.append(nn.MaxPool2d(kernel_size=2))\n",
        "      inChannels = self.num_filters[i]\n",
        "\n",
        "    # computing flattened size\n",
        "    input_shape = (3, 224, 224)\n",
        "    with torch.no_grad():\n",
        "      dummy_input = torch.zeros(1, *input_shape)\n",
        "      dummy_output = self.model(dummy_input)\n",
        "      flattened_size = dummy_output.view(dummy_output.size(0), -1).size(1)\n",
        "\n",
        "    self.model.append(nn.Flatten())\n",
        "    self.model.append(nn.Linear(flattened_size, self.n_neurons_denseLayer))\n",
        "    self.model.append(self.activationFun())\n",
        "\n",
        "    if(self.dropout > 0):\n",
        "      self.model.append(nn.Dropout(self.dropout))\n",
        "\n",
        "    self.model.append(nn.Linear(self.n_neurons_denseLayer, self.num_classes))\n",
        "\n",
        "  def forward(self, inputs):\n",
        "    return self.model(inputs)\n",
        "\n",
        "  def backward(self, outputs, labels):\n",
        "    loss = nn.CrossEntropyLoss()(outputs, labels)\n",
        "    loss.backward()\n",
        "\n",
        "  def updateWeights(self):\n",
        "    self.optimizer.step()"
      ],
      "metadata": {
        "id": "RVUtdV4T2bAx"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training CNN"
      ],
      "metadata": {
        "id": "eHcPcof_tmdc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Global variable to store the path to the best model\n",
        "best_model_path = '/content/drive/MyDrive/DL_Assignment2/best_model.pth'\n",
        "best_val_accuracy = 0.0\n",
        "\n",
        "sweep_configuration = {\n",
        "    \"method\": \"bayes\",\n",
        "    \"name\" : \"train_sweep\",\n",
        "    \"metric\": {\"name\": \"val_accuracy\", \"goal\": \"maximize\"},\n",
        "    \"parameters\": {\n",
        "        \"num_filters\": {'values': [[32, 32, 32, 32, 32], [32, 64, 64, 128, 256], [256, 128, 64, 64, 32]]},\n",
        "        \"filter_sizes\": {'values': [[3, 3, 3, 3, 3], [5, 5, 5, 5, 5]]},\n",
        "        \"activation\": {\"values\": [\"ReLU\", \"SiLU\", \"GELU\"]},\n",
        "        \"optimizer\": {\"values\": [\"adam\", \"rmsprop\", \"sgd\"]},\n",
        "        \"learning_rate\": {\"values\": [1e-3]},\n",
        "        \"weight_decay\": {\"values\": [0.0001]},\n",
        "        \"momentum\": {\"values\": [0.9]},\n",
        "        \"beta\": {\"values\": [0.9]},\n",
        "        \"beta1\": {\"values\":[0.9]},\n",
        "        \"beta2\": {\"values\": [0.999]},\n",
        "        \"epsilon\": {\"values\": [1e-8]},\n",
        "        \"base_dir\": {\"values\":[\"/content/drive/MyDrive/DL_Assignment2/Dataset/inaturalist_12K/\"]},\n",
        "        \"isDataAug\": {\"values\": [\"False\", \"True\"]},\n",
        "        \"isBatchNormalization\": {\"values\": [\"True\", \"False\"]},\n",
        "        \"dropout\": {\"values\": [0.2, 0.3]},\n",
        "        \"n_neurons_denseLayer\": {\"values\": [128]}\n",
        "    }\n",
        "}\n",
        "\n",
        "\n",
        "def findOutputs(cnn, inputDataLoader):\n",
        "  cnn.eval()  # setting the model to evaluation model\n",
        "  outputs = []\n",
        "  total_loss = 0.0\n",
        "  n_correct = 0\n",
        "  n_samples = 0\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for batch_idx, (x_batch, y_batch) in enumerate(inputDataLoader):\n",
        "      x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
        "      batch_outputs = cnn(x_batch)\n",
        "\n",
        "      loss = nn.CrossEntropyLoss()(batch_outputs, y_batch)\n",
        "      total_loss += loss.item() * x_batch.size(0)\n",
        "\n",
        "      y_pred_batch = torch.argmax(batch_outputs, dim=1)\n",
        "      n_correct += (y_pred_batch == y_batch).sum().item()\n",
        "      n_samples += x_batch.size(0)\n",
        "\n",
        "      outputs.append(batch_outputs)\n",
        "\n",
        "  outputs = torch.cat(outputs)\n",
        "  accuracy = (n_correct * 100.0) / n_samples\n",
        "  avg_loss = total_loss / n_samples\n",
        "  return outputs, accuracy, avg_loss\n",
        "\n",
        "def trainNeuralNetwork_sweep():\n",
        "  wandb.init(mode=\"online\")\n",
        "  args = wandb.config\n",
        "  train_loader, test_loader, val_loader, num_classes = load_data(args[\"base_dir\"], args[\"isDataAug\"])\n",
        "  activationFun = args[\"activation\"]\n",
        "  optimizer = args[\"optimizer\"]\n",
        "  learning_rate = args[\"learning_rate\"]\n",
        "  momentum = args[\"momentum\"]\n",
        "  beta = args[\"beta\"]\n",
        "  beta1 = args[\"beta1\"]\n",
        "  beta2 = args[\"beta2\"]\n",
        "  epsilon = args[\"epsilon\"]\n",
        "  weight_decay = args[\"weight_decay\"]\n",
        "  dropout = args[\"dropout\"]\n",
        "  num_filters = args[\"num_filters\"]\n",
        "  filter_sizes = args[\"filter_sizes\"]\n",
        "  n_neurons_denseLayer = args[\"n_neurons_denseLayer\"]\n",
        "  isBatchNormalization = args[\"isBatchNormalization\"]\n",
        "  isDataAug = args[\"isDataAug\"]\n",
        "\n",
        "  wandb.run.name = f\"{activationFun}_{optimizer}_{dropout}_{n_neurons_denseLayer}_DataAug-{isDataAug}_BatchNorm-{isBatchNormalization}\"\n",
        "\n",
        "  cnn = ConvolutionalNeuralNetwork(num_classes,\n",
        "                                   num_filters, filter_sizes,\n",
        "                                   activationFun, optimizer,\n",
        "                                   n_neurons_denseLayer,\n",
        "                                   isBatchNormalization, dropout,\n",
        "                                   learning_rate,\n",
        "                                   momentum, beta,\n",
        "                                   beta1, beta2,\n",
        "                                   epsilon, weight_decay)\n",
        "  cnn.to(device)\n",
        "\n",
        "  epochs = 10\n",
        "  for epochNum in range(epochs):\n",
        "    print(f\"Epoch {epochNum}:\")\n",
        "    for batch_idx, (x_batch, y_batch) in enumerate(train_loader):\n",
        "      print(f\"Batch idx {batch_idx} running\")\n",
        "      x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
        "      cnn.optimizer.zero_grad()\n",
        "      outputs = cnn(x_batch)\n",
        "      cnn.backward(outputs, y_batch)\n",
        "      cnn.updateWeights()\n",
        "\n",
        "    # Validation accuracy\n",
        "    val_outputs, val_accuracy, val_loss = findOutputs(cnn, val_loader)\n",
        "    wandb.run.summary[\"metric_name\"] = val_accuracy\n",
        "    print(f\"validation: loss={val_loss}, accuracy={val_accuracy}\")\n",
        "\n",
        "    # Train accuracy\n",
        "    train_outputs, train_accuracy, train_loss = findOutputs(cnn, train_loader)\n",
        "    print(f\"training: loss={val_loss}, training={val_accuracy}\")\n",
        "\n",
        "    if val_accuracy > best_val_accuracy:\n",
        "      best_val_accuracy = val_accuracy\n",
        "      torch.save(cnn.state_dict(), best_model_path)  # Save the best model\n",
        "      wandb.run.summary[\"best_model_path\"] = best_model_path  # Log the model path in W&B\n",
        "      wandb.run.summary[\"best_val_accuracy\"] = best_val_accuracy\n",
        "\n",
        "    wandb.log({\n",
        "        \"epoch\": epochNum + 1,\n",
        "        \"validation_loss\": val_loss,\n",
        "        \"validation_accuracy\": val_accuracy,\n",
        "        \"train_loss\": train_loss,\n",
        "        \"train_accuracy\": train_accuracy\n",
        "        },commit=True)\n",
        "\n",
        "  wandb.finish()"
      ],
      "metadata": {
        "id": "zEGF79uTtolO"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.login()\n",
        "wandb_id = wandb.sweep(sweep_configuration, project=\"DA6401_Assignment2\")\n",
        "wandb.agent(wandb_id, function=trainNeuralNetwork_sweep)"
      ],
      "metadata": {
        "id": "RIw7bCO8CUU7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}