{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Installation\n",
        "\n"
      ],
      "metadata": {
        "id": "vGLMBeNg-CHh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TDRNq2wV9Yxr",
        "outputId": "13fa649c-b3c5-419d-c9f0-eecbd78177e8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: wandb in /usr/local/lib/python3.11/dist-packages (0.19.9)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\n",
            "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.11/dist-packages (from wandb) (8.1.8)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (3.1.44)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.11/dist-packages (from wandb) (4.3.7)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (5.29.4)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (5.9.5)\n",
            "Requirement already satisfied: pydantic<3 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.11.3)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from wandb) (6.0.2)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.32.3)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.25.1)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.11/dist-packages (from wandb) (1.3.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from wandb) (75.2.0)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4 in /usr/local/lib/python3.11/dist-packages (from wandb) (4.13.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.57.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.17.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (2.33.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (0.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (2025.1.31)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m52.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m35.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m64.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m94.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n"
          ]
        }
      ],
      "source": [
        "pip install wandb numpy pandas matplotlib torch torchvision"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset"
      ],
      "metadata": {
        "id": "2pKvVtLV-TqH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zftJw42q-LCj",
        "outputId": "2b934665-1770-453f-8125-93646a389ee5"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!unzip /content/drive/MyDrive/DL_Assignment2/Dataset/nature_12K.zip -d /content/drive/MyDrive/DL_Assignment2/Dataset"
      ],
      "metadata": {
        "id": "Vk6BRhrcAydF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Libraries"
      ],
      "metadata": {
        "id": "4cSWu4KFCJLE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import os\n",
        "from torchvision import datasets, transforms\n",
        "from sklearn.model_selection import train_test_split, StratifiedShuffleSplit\n",
        "from torch.utils.data import Subset, DataLoader\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import wandb\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "6a0b6a18CMBf"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset loader"
      ],
      "metadata": {
        "id": "lF5ZbaMoGFGm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def validationDataSplit(train_dataset):\n",
        "  classLabels = [label for _,label in train_dataset.samples]\n",
        "  num_classes = len(np.unique(classLabels))\n",
        "\n",
        "  sss = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
        "  train_indices, val_indices = next(sss.split(train_dataset.samples, classLabels))\n",
        "\n",
        "  train_subset = Subset(train_dataset, train_indices)\n",
        "  val_subset = Subset(train_dataset, val_indices)\n",
        "  return train_subset, val_subset, num_classes\n",
        "\n",
        "\n",
        "def load_data(base_dir, isDataAug):\n",
        "  train_dir = os.path.join(base_dir, 'train')\n",
        "  test_dir = os.path.join(base_dir, 'val')\n",
        "\n",
        "  train_transform, test_transform = None, None\n",
        "\n",
        "  if isDataAug == False:\n",
        "    train_transform = transforms.Compose([\n",
        "      transforms.Resize(256),\n",
        "      transforms.CenterCrop(224),\n",
        "      transforms.ToTensor(),\n",
        "      transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])\n",
        "  else:\n",
        "    train_transform = transforms.Compose([\n",
        "      transforms.Resize(256),\n",
        "      transforms.CenterCrop(224),\n",
        "      transforms.RandomHorizontalFlip(),\n",
        "      transforms.RandomRotation(10),\n",
        "      transforms.ToTensor(),\n",
        "      transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])\n",
        "\n",
        "  test_transform = transforms.Compose([\n",
        "        transforms.Resize(256),\n",
        "        transforms.CenterCrop(224),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])\n",
        "\n",
        "  train_dataset = datasets.ImageFolder(train_dir, transform=train_transform)\n",
        "  test_dataset = datasets.ImageFolder(test_dir, transform=test_transform)\n",
        "  train_dataset, val_dataset, num_classes = validationDataSplit(train_dataset)\n",
        "\n",
        "  # print(f\"inp: {train_dataset[0][0].shape} {train_dataset[0][1]}\")\n",
        "\n",
        "  train_loader = DataLoader(train_dataset,shuffle=True,num_workers=2,batch_size=64,pin_memory=True)\n",
        "  test_loader = DataLoader(test_dataset,shuffle=True,num_workers=2,batch_size=64,pin_memory=True)\n",
        "  val_loader = DataLoader(val_dataset,shuffle=True,num_workers=2,batch_size=64,pin_memory=True)\n",
        "\n",
        "  return train_loader, test_loader, val_loader, num_classes\n",
        "\n",
        "# load_data(\"/content/drive/MyDrive/DL_Assignment2/Dataset/inaturalist_12K/\", True)"
      ],
      "metadata": {
        "id": "17mDOMKWGIGJ"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Network"
      ],
      "metadata": {
        "id": "5T2fCf1l2ZJF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ConvolutionalNeuralNetwork(nn.Module):\n",
        "  activationFunctionsMap = {\"ReLU\": nn.ReLU, \"GELU\": nn.GELU, \"SiLU\": nn.SiLU}\n",
        "  # optimizersMap = {\"sgd\": optim.SGD, \"rmsprop\": optim.RMSprop, \"adam\": optim.Adam}\n",
        "\n",
        "  def __init__(self, num_classes,\n",
        "               num_filters, filter_sizes,\n",
        "               activationFun, optimizer,\n",
        "               n_neurons_denseLayer,\n",
        "               isBatchNormalization, dropout,\n",
        "               learning_rate=0.001,\n",
        "               momentum=0.5, beta = 0.9,\n",
        "               beta1=0.9, beta2=0.99,\n",
        "               epsilon=1e-8, weight_decay=0.0001):\n",
        "    super(ConvolutionalNeuralNetwork, self).__init__()\n",
        "    self.num_classes = num_classes\n",
        "    self.num_filters = num_filters\n",
        "    self.filter_sizes = filter_sizes\n",
        "    self.activationFun = ConvolutionalNeuralNetwork.activationFunctionsMap[activationFun]\n",
        "    # self.optimizer = ConvolutionalNeuralNetwork.optimizersMap[optimizer]\n",
        "\n",
        "    self.n_neurons_denseLayer = n_neurons_denseLayer\n",
        "    self.isBatchNormalization = isBatchNormalization\n",
        "    self.dropout = dropout\n",
        "\n",
        "    self.lr = learning_rate\n",
        "    self.momentum = momentum\n",
        "    self.betas = (beta1, beta2)\n",
        "    self.eps = epsilon\n",
        "    self.alpha = self.beta\n",
        "    self.weight_decay = weight_decay\n",
        "\n",
        "    if(optimizer == \"sgd\"):\n",
        "      self.optimizer = optim.SGD(self.parameters(), lr=self.lr, momentum=self.momentum, weight_decay=self.weight_decay)\n",
        "    elif(optimizer == \"rmsprop\"):\n",
        "      self.optimizer = optim.RMSprop(self.parameters(), lr=self.lr, alpha=self.alpha, eps=self.eps, weight_decay=self.weight_decay)\n",
        "    elif(optimizer == \"adam\"):\n",
        "      self.optimizer = optim.Adam(self.parameters(), lr=self.lr, betas=self.betas, eps=self.eps, weight_decay=self.weight_decay)\n",
        "\n",
        "    self.defineModel()\n",
        "\n",
        "\n",
        "  def defineModel(self):\n",
        "    self.model = nn.Sequential()\n",
        "\n",
        "    inChannels = 3;     # RGB channels for inaturalist\n",
        "    for i in range(len(self.num_filters)):\n",
        "      self.model.append(nn.Conv2d(inChannels, self.num_filters[i], self.filter_sizes[i], padding=self.filter_sizes[i]//2))\n",
        "      if self.isBatchNormalization:\n",
        "        self.model.append(nn.BatchNorm2d(self.num_filters[i]))\n",
        "      self.model.append(self.activationFun())\n",
        "      self.model.append(nn.MaxPool2d(kernel_size=2))\n",
        "      inChannels = self.num_filters[i]\n",
        "\n",
        "    # computing flattened size\n",
        "    input_shape = (3, 224, 224)\n",
        "    with torch.no_grad():\n",
        "      dummy_input = torch.zeros(1, *input_shape)\n",
        "      dummy_output = self.model(dummy_input)\n",
        "      flattened_size = dummy_output.view(dummy_output.size(0), -1).size(1)\n",
        "\n",
        "    self.model.append(nn.Flatten())\n",
        "    self.model.append(nn.Linear(flattened_size, self.n_neurons_denseLayer))\n",
        "    self.model.append(self.activationFun())\n",
        "\n",
        "    if(self.dropout > 0):\n",
        "      self.model.append(nn.Dropout(self.dropout))\n",
        "\n",
        "    self.model.append(nn.Linear(self.n_neurons_denseLayer, self.num_classes))\n",
        "\n",
        "    return self.model\n",
        "\n",
        "  def forward(self, inputs):\n",
        "    return self.model(inputs)\n",
        "\n",
        "  def backward(self, outputs, labels):\n",
        "    loss = nn.CrossEntropyLoss(outputs, labels)\n",
        "    loss.backward()\n",
        "\n",
        "  def updateWeights(self):\n",
        "    self.optimizer.step()"
      ],
      "metadata": {
        "id": "RVUtdV4T2bAx"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training CNN"
      ],
      "metadata": {
        "id": "eHcPcof_tmdc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Global variable to store the path to the best model\n",
        "best_model_path = '/content/drive/MyDrive/DL_Assignment2/best_model.pth'\n",
        "best_val_accuracy = 0.0\n",
        "\n",
        "sweep_configuration = {\n",
        "    \"method\": \"bayes\",\n",
        "    \"name\" : \"train_sweep\",\n",
        "    \"metric\": {\"name\": \"validation_accuracy\", \"goal\": \"maximize\"},\n",
        "    \"parameters\": {\n",
        "        \"num_filters\": {'values': [[32, 32, 32, 32, 32], [32, 64, 64, 128, 256], [256, 128, 64, 64, 32]]},\n",
        "        \"filter_sizes\": {'values': [[3, 3, 3, 3, 3], [5, 5, 5, 5, 5]]},\n",
        "        \"activation\": {\"values\": [\"ReLU\", \"SiLU\", \"GELU\"]},\n",
        "        \"optimizer\": {\"values\": [\"adam\", \"rmsprop\", \"sgd\"]},\n",
        "        \"learning_rate\": {\"values\": [1e-3]},\n",
        "        \"weight_decay\": {\"values\": [0.0001]},\n",
        "        \"momentum\": {\"values\": [0.9]},\n",
        "        \"beta\": {\"values\": [0.9]},\n",
        "        \"beta1\": {\"values\":[0.9]},\n",
        "        \"beta2\": {\"values\": [0.999]},\n",
        "        \"epsilon\": {\"values\": [1e-8]},\n",
        "        \"base_dir\": {\"values\":[\"/content/drive/MyDrive/DL_Assignment2/Dataset/inaturalist_12K/\"]},\n",
        "        \"isDataAug\": {\"values\": [\"False\", \"True\"]},\n",
        "        \"isBatchNormalization\": {\"values\": [\"True\", \"False\"]},\n",
        "        \"dropout\": {\"values\": [0.2, 0.3]},\n",
        "        \"n_neurons_denseLayer\": {\"values\": [128]}\n",
        "    }\n",
        "}\n",
        "\n",
        "\n",
        "def findOutputs(cnn, inputDataLoader):\n",
        "  cnn.eval()  # setting the model to evaluation model\n",
        "  outputs = []\n",
        "  total_loss = 0.0\n",
        "  n_correct = 0\n",
        "  n_samples = 0\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for batch_idx, (x_batch, y_batch) in enumerate(inputDataLoader):\n",
        "      x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
        "      batch_outputs = cnn(x_batch)\n",
        "\n",
        "      loss = nn.CrossEntropyLoss(batch_outputs, y_batch)\n",
        "      total_loss += loss.item() * x_batch.size(0)\n",
        "\n",
        "      y_pred_batch = torch.argmax(batch_outputs, dim=1)\n",
        "      n_correct += (y_pred_batch == y_batch).sum().item()\n",
        "      n_samples += x_batch.size(0)\n",
        "\n",
        "      outputs.append(batch_outputs)\n",
        "\n",
        "  outputs = torch.cat(outputs)\n",
        "  accuracy = (n_correct * 100.0) / n_samples\n",
        "  avg_loss = total_loss / n_samples\n",
        "  return outputs, accuracy, avg_loss\n",
        "\n",
        "def trainNeuralNetwork_sweep():\n",
        "  wandb.init(mode=\"online\")\n",
        "  args = wandb.config\n",
        "  train_loader, test_loader, val_loader, num_classes = load_data(args[\"base_dir\"], args[\"isDataAug\"])\n",
        "  activationFun = args[\"activation\"]\n",
        "  optimizer = args[\"optimizer\"]\n",
        "  learning_rate = args[\"learning_rate\"]\n",
        "  momentum = args[\"momentum\"]\n",
        "  beta = args[\"beta\"]\n",
        "  beta1 = args[\"beta1\"]\n",
        "  beta2 = args[\"beta2\"]\n",
        "  epsilon = args[\"epsilon\"]\n",
        "  weight_decay = args[\"weight_decay\"]\n",
        "  dropout = args[\"dropout\"]\n",
        "  num_filters = args[\"num_filters\"]\n",
        "  filter_sizes = args[\"filter_sizes\"]\n",
        "  n_neurons_denseLayer = args[\"n_neurons_denseLayer\"]\n",
        "  isBatchNormalization = args[\"isBatchNormalization\"]\n",
        "  isDataAug = args[\"isDataAug\"]\n",
        "\n",
        "  wandb.run.name = f\"{activationFun}_{optimizer}_{dropout}_{n_neurons_denseLayer}_DataAug-{isDataAug}_BatchNorm-{isBatchNormalization}\"\n",
        "\n",
        "  cnn = ConvolutionalNeuralNetwork(num_classes,\n",
        "                                   num_filters, filter_sizes,\n",
        "                                   activationFun, optimizer,\n",
        "                                   n_neurons_denseLayer,\n",
        "                                   isBatchNormalization, dropout,\n",
        "                                   learning_rate,\n",
        "                                   momentum, beta,\n",
        "                                   beta1, beta2,\n",
        "                                   epsilon, weight_decay)\n",
        "  cnn.to(device)\n",
        "\n",
        "  epochs = 10\n",
        "  for epochNum in range(epochs):\n",
        "    for batch_idx, (x_batch, y_batch) in enumerate(train_loader):\n",
        "      x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
        "      cnn.optimizer.zero_grad()\n",
        "      outputs = cnn(x_batch)\n",
        "      cnn.backward(outputs, y_batch)\n",
        "      cnn.updateWeights()\n",
        "\n",
        "    # Validation accuracy\n",
        "    val_outputs, val_accuracy, val_loss = findOutputs(cnn, val_loader)\n",
        "    wandb.run.summary[\"metric_name\"] = val_accuracy\n",
        "\n",
        "    # Train accuracy\n",
        "    train_outputs, train_accuracy, train_loss = findOutputs(cnn, train_loader)\n",
        "\n",
        "    if val_accuracy > best_val_accuracy:\n",
        "      best_val_accuracy = val_accuracy\n",
        "      torch.save(cnn.state_dict(), best_model_path)  # Save the best model\n",
        "      wandb.run.summary[\"best_model_path\"] = best_model_path  # Log the model path in W&B\n",
        "      wandb.run.summary[\"best_val_accuracy\"] = best_val_accuracy\n",
        "\n",
        "    wandb.log({\n",
        "        \"epoch\": epochNum + 1,\n",
        "        \"validation_loss\": val_loss,\n",
        "        \"validation_accuracy\": val_accuracy,\n",
        "        \"train_loss\": train_loss,\n",
        "        \"train_accuracy\": train_accuracy\n",
        "        },commit=True)\n",
        "\n",
        "  wandb.finish()"
      ],
      "metadata": {
        "id": "zEGF79uTtolO"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.login()\n",
        "wandb_id = wandb.sweep(sweep_configuration, project=\"DA6401_Assignment2\")\n",
        "wandb.agent(wandb_id, function=trainNeuralNetwork_sweep)"
      ],
      "metadata": {
        "id": "RIw7bCO8CUU7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}