{
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.11",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU",
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "sourceId": 11450933,
          "sourceType": "datasetVersion",
          "datasetId": 7174491
        },
        {
          "sourceId": 11461644,
          "sourceType": "datasetVersion",
          "datasetId": 7182005
        }
      ],
      "dockerImageVersionId": 31011,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4EQAQNrB5CmG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Installation\n",
        "\n"
      ],
      "metadata": {
        "id": "vGLMBeNg-CHh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install wandb numpy pandas matplotlib torch torchvision"
      ],
      "metadata": {
        "id": "TDRNq2wV9Yxr"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset"
      ],
      "metadata": {
        "id": "2pKvVtLV-TqH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zftJw42q-LCj",
        "outputId": "311f8a3c-81bd-41ec-99ac-8f18b2988294"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "execution_count": 2
    },
    {
      "cell_type": "code",
      "source": [
        "# !unzip /content/drive/MyDrive/DL_Assignment2/Dataset/nature_12K.zip -d /content/drive/MyDrive/DL_Assignment2/Dataset"
      ],
      "metadata": {
        "id": "Vk6BRhrcAydF",
        "collapsed": true,
        "jupyter": {
          "outputs_hidden": true
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.chdir(\"/content/drive/MyDrive/DL_Assignment2\")"
      ],
      "metadata": {
        "id": "uZD5dVKBPKJg"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Libraries"
      ],
      "metadata": {
        "id": "4cSWu4KFCJLE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile libraries.py\n",
        "import torch\n",
        "import os\n",
        "from torchvision import datasets, transforms\n",
        "from sklearn.model_selection import train_test_split, StratifiedShuffleSplit\n",
        "from torch.utils.data import Subset, DataLoader\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import wandb\n",
        "import tqdm\n",
        "import gc\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "6a0b6a18CMBf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ccfc9e79-0970-402a-f688-0cd6c641b5ca",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-19T16:20:35.015312Z",
          "iopub.execute_input": "2025-04-19T16:20:35.015580Z",
          "iopub.status.idle": "2025-04-19T16:20:35.021081Z",
          "shell.execute_reply.started": "2025-04-19T16:20:35.015557Z",
          "shell.execute_reply": "2025-04-19T16:20:35.020335Z"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting libraries.py\n"
          ]
        }
      ],
      "execution_count": 21
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset loader"
      ],
      "metadata": {
        "id": "lF5ZbaMoGFGm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile data_loader.py\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import Subset, DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "from sklearn.model_selection import train_test_split, StratifiedShuffleSplit\n",
        "import os\n",
        "\n",
        "\n",
        "def validationDataSplit(train_dataset):\n",
        "  classLabels = [label for _,label in train_dataset.samples]\n",
        "  num_classes = len(np.unique(classLabels))\n",
        "\n",
        "  sss = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
        "  train_indices, val_indices = next(sss.split(train_dataset.samples, classLabels))\n",
        "\n",
        "  train_subset = Subset(train_dataset, train_indices)\n",
        "  val_subset = Subset(train_dataset, val_indices)\n",
        "  return train_subset, val_subset, num_classes\n",
        "\n",
        "\n",
        "def load_data(base_dir, isDataAug, batch_size):\n",
        "  train_dir = os.path.join(base_dir, 'train')\n",
        "  test_dir = os.path.join(base_dir, 'val')\n",
        "\n",
        "  train_transform, test_transform = None, None\n",
        "\n",
        "  if isDataAug == False:\n",
        "    train_transform = transforms.Compose([\n",
        "      transforms.Resize(256),\n",
        "      transforms.CenterCrop(224),\n",
        "      transforms.ToTensor(),\n",
        "      transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])\n",
        "  else:\n",
        "    train_transform = transforms.Compose([\n",
        "      transforms.Resize(256),\n",
        "      transforms.CenterCrop(224),\n",
        "      transforms.RandomHorizontalFlip(),\n",
        "      transforms.RandomRotation(10),\n",
        "      transforms.ToTensor(),\n",
        "      transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])\n",
        "\n",
        "  test_transform = transforms.Compose([\n",
        "        transforms.Resize(256),\n",
        "        transforms.CenterCrop(224),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])\n",
        "\n",
        "  train_dataset = datasets.ImageFolder(train_dir, transform=train_transform)\n",
        "  test_dataset = datasets.ImageFolder(test_dir, transform=test_transform)\n",
        "  train_dataset, val_dataset, num_classes = validationDataSplit(train_dataset)\n",
        "\n",
        "  # print(f\"inp: {train_dataset[0][0].shape} {train_dataset[0][1]}\")\n",
        "\n",
        "  train_loader = DataLoader(train_dataset,shuffle=True,num_workers=2,batch_size=batch_size,pin_memory=True)\n",
        "  test_loader = DataLoader(test_dataset,shuffle=True,num_workers=2,batch_size=64,pin_memory=True)\n",
        "  val_loader = DataLoader(val_dataset,shuffle=True,num_workers=2,batch_size=64,pin_memory=True)\n",
        "\n",
        "  return train_loader, test_loader, val_loader, num_classes\n",
        "\n",
        "# load_data(\"/content/drive/MyDrive/DL_Assignment2/Dataset/inaturalist_12K/\", True)"
      ],
      "metadata": {
        "id": "17mDOMKWGIGJ",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-19T16:20:38.996239Z",
          "iopub.execute_input": "2025-04-19T16:20:38.996931Z",
          "iopub.status.idle": "2025-04-19T16:20:39.004844Z",
          "shell.execute_reply.started": "2025-04-19T16:20:38.996906Z",
          "shell.execute_reply": "2025-04-19T16:20:39.004101Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "479d8ece-590b-46ee-e154-4cfb6e766e30"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting data_loader.py\n"
          ]
        }
      ],
      "execution_count": 22
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training CNN"
      ],
      "metadata": {
        "id": "YFLtsBU3CanP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile neural_network.py\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch\n",
        "\n",
        "class ConvolutionalNeuralNetwork(nn.Module):\n",
        "  activationFunctionsMap = {\"ReLU\": nn.ReLU, \"GELU\": nn.GELU, \"SiLU\": nn.SiLU}\n",
        "  # optimizersMap = {\"sgd\": optim.SGD, \"rmsprop\": optim.RMSprop, \"adam\": optim.Adam}\n",
        "\n",
        "  def __init__(self, num_classes,\n",
        "               num_filters, filter_sizes,\n",
        "               activationFun, optimizer,\n",
        "               n_neurons_denseLayer,\n",
        "               isBatchNormalization, dropout,\n",
        "               learning_rate=0.001,\n",
        "               momentum=0.5, beta = 0.9,\n",
        "               beta1=0.9, beta2=0.99,\n",
        "               epsilon=1e-8, weight_decay=0.0001):\n",
        "    super(ConvolutionalNeuralNetwork, self).__init__()\n",
        "    self.num_classes = num_classes\n",
        "    self.num_filters = num_filters\n",
        "    self.filter_sizes = filter_sizes\n",
        "    self.activationFun = ConvolutionalNeuralNetwork.activationFunctionsMap[activationFun]\n",
        "    # self.optimizer = ConvolutionalNeuralNetwork.optimizersMap[optimizer]\n",
        "\n",
        "    self.n_neurons_denseLayer = n_neurons_denseLayer\n",
        "    self.isBatchNormalization = isBatchNormalization\n",
        "    self.dropout = dropout\n",
        "\n",
        "    self.lr = learning_rate\n",
        "    self.momentum = momentum\n",
        "    self.betas = (beta1, beta2)\n",
        "    self.eps = epsilon\n",
        "    self.alpha = beta\n",
        "    self.weight_decay = weight_decay\n",
        "\n",
        "    self.defineModel()\n",
        "\n",
        "    if(optimizer == \"sgd\"):\n",
        "      self.optimizer = optim.SGD(self.parameters(), lr=self.lr, momentum=self.momentum, weight_decay=self.weight_decay)\n",
        "    elif(optimizer == \"rmsprop\"):\n",
        "      self.optimizer = optim.RMSprop(self.parameters(), lr=self.lr, alpha=self.alpha, eps=self.eps, weight_decay=self.weight_decay)\n",
        "    elif(optimizer == \"adam\"):\n",
        "      self.optimizer = optim.Adam(self.parameters(), lr=self.lr, betas=self.betas, eps=self.eps, weight_decay=self.weight_decay)\n",
        "\n",
        "\n",
        "\n",
        "  def defineModel(self):\n",
        "    self.model = nn.Sequential()\n",
        "\n",
        "    inChannels = 3;     # RGB channels for inaturalist\n",
        "    for i in range(len(self.num_filters)):\n",
        "      self.model.append(nn.Conv2d(inChannels, self.num_filters[i], self.filter_sizes[i], padding=self.filter_sizes[i]//2))\n",
        "      if self.isBatchNormalization:\n",
        "        self.model.append(nn.BatchNorm2d(self.num_filters[i]))\n",
        "      self.model.append(self.activationFun())\n",
        "      self.model.append(nn.MaxPool2d(kernel_size=2))\n",
        "      inChannels = self.num_filters[i]\n",
        "\n",
        "    # computing flattened size\n",
        "    input_shape = (3, 224, 224)\n",
        "    with torch.no_grad():\n",
        "      dummy_input = torch.zeros(1, *input_shape)\n",
        "      dummy_output = self.model(dummy_input)\n",
        "      flattened_size = dummy_output.view(dummy_output.size(0), -1).size(1)\n",
        "\n",
        "    self.model.append(nn.Flatten())\n",
        "    self.model.append(nn.Linear(flattened_size, self.n_neurons_denseLayer))\n",
        "    self.model.append(self.activationFun())\n",
        "\n",
        "    if(self.dropout > 0):\n",
        "      self.model.append(nn.Dropout(self.dropout))\n",
        "\n",
        "    self.model.append(nn.Linear(self.n_neurons_denseLayer, self.num_classes))\n",
        "\n",
        "  def forward(self, inputs):\n",
        "    return self.model(inputs)\n",
        "\n",
        "  def backward(self, outputs, labels):\n",
        "    loss = nn.CrossEntropyLoss()(outputs, labels)\n",
        "    loss.backward()\n",
        "\n",
        "  def updateWeights(self):\n",
        "    self.optimizer.step()"
      ],
      "metadata": {
        "id": "RVUtdV4T2bAx",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-19T16:20:45.770129Z",
          "iopub.execute_input": "2025-04-19T16:20:45.770405Z",
          "iopub.status.idle": "2025-04-19T16:20:45.781208Z",
          "shell.execute_reply.started": "2025-04-19T16:20:45.770384Z",
          "shell.execute_reply": "2025-04-19T16:20:45.780383Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "914d0982-2b74-4c02-d29e-5a92b9591d55"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting neural_network.py\n"
          ]
        }
      ],
      "execution_count": 23
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Accuracy calculation"
      ],
      "metadata": {
        "id": "etvm8-FJHS7e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile accuracy_calculation.py\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Subset, DataLoader\n",
        "\n",
        "def findOutputs(device, cnn, inputDataLoader, isTestData=False):\n",
        "  cnn.eval()  # setting the model to evaluation model\n",
        "  outputs = []\n",
        "  total_loss = 0.0\n",
        "  n_correct = 0\n",
        "  n_correct_top5 = 0\n",
        "  n_correct_top2 = 0\n",
        "  n_samples = 0\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for batch_idx, (x_batch, y_batch) in enumerate(inputDataLoader):\n",
        "      x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
        "      batch_outputs = cnn(x_batch)\n",
        "\n",
        "      loss = nn.CrossEntropyLoss()(batch_outputs, y_batch)\n",
        "      total_loss += loss.item() * x_batch.size(0)\n",
        "\n",
        "      y_pred_batch = torch.argmax(batch_outputs, dim=1)\n",
        "      n_correct += (y_pred_batch == y_batch).sum().item()\n",
        "      n_samples += x_batch.size(0)\n",
        "\n",
        "      if isTestData == True:\n",
        "          y_pred_batch_top5 = torch.topk(batch_outputs, 5, dim=1).indices\n",
        "          n_correct_top5 += y_pred_batch_top5.eq(y_batch.view(-1, 1)).sum().item()\n",
        "\n",
        "          y_pred_batch_top2 = torch.topk(batch_outputs, 2, dim=1).indices\n",
        "          n_correct_top2 += y_pred_batch_top2.eq(y_batch.view(-1, 1)).sum().item()\n",
        "      outputs.append(batch_outputs)\n",
        "\n",
        "  outputs = torch.cat(outputs)\n",
        "  accuracy = (n_correct * 100.0) / n_samples\n",
        "  avg_loss = total_loss / n_samples\n",
        "\n",
        "  top5_accuracy = None\n",
        "  top2_accuracy = None\n",
        "  if isTestData == True:\n",
        "      top5_accuracy = (n_correct_top5 * 100.0) / n_samples\n",
        "      top2_accuracy = (n_correct_top2 * 100.0) / n_samples\n",
        "  return outputs, accuracy, avg_loss, top5_accuracy, top2_accuracy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lbXpvHQ8HV9N",
        "outputId": "1c875e54-3d87-4a67-aa20-d38fd5c2bfe8"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting accuracy_calculation.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training (Argparser included)"
      ],
      "metadata": {
        "id": "EEUJ3MT6FuqS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile train_local.py\n",
        "import os\n",
        "import gc\n",
        "import wandb\n",
        "import torch\n",
        "from neural_network import *\n",
        "from data_loader import *\n",
        "from accuracy_calculation import *\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)\n",
        "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
        "\n",
        "def trainNeuralNetwork_local(args):\n",
        "  wandb.login()\n",
        "  wandb.init(mode=\"online\")\n",
        "  wandb.init(project=args.wandb_project, entity=args.wandb_entity)\n",
        "  if args.isDataAug == \"True\":\n",
        "    isDataAug = True\n",
        "  else:\n",
        "    isDataAug = False\n",
        "\n",
        "  if args.isBatchNormalization == \"True\":\n",
        "    isBatchNormalization = True\n",
        "  else:\n",
        "    isBatchNormalization = False\n",
        "\n",
        "  train_loader, test_loader, val_loader, num_classes = load_data(args.base_dir, isDataAug, args.batch_size)\n",
        "  activationFun = args.activation\n",
        "  optimizer = args.optimizer\n",
        "  learning_rate = args.learning_rate\n",
        "  momentum = args.momentum\n",
        "  beta = args.beta\n",
        "  beta1 = args.beta1\n",
        "  beta2 = args.beta2\n",
        "  epsilon = args.epsilon\n",
        "  weight_decay = args.weight_decay\n",
        "  dropout = args.dropout\n",
        "  num_filters = args.num_filters\n",
        "  filter_sizes = args.filter_sizes\n",
        "  n_neurons_denseLayer = args.n_neurons_denseLayer\n",
        "\n",
        "  wandb.run.name = f\"{activationFun}_{optimizer}_{dropout}_{n_neurons_denseLayer}_DataAug-{isDataAug}_BatchNorm-{isBatchNormalization}\"\n",
        "  best_val_accuracy = 0.0\n",
        "  best_accuracy_epoch = -1\n",
        "\n",
        "  cnn = ConvolutionalNeuralNetwork(num_classes,\n",
        "                                   num_filters, filter_sizes,\n",
        "                                   activationFun, optimizer,\n",
        "                                   n_neurons_denseLayer,\n",
        "                                   isBatchNormalization, dropout,\n",
        "                                   learning_rate,\n",
        "                                   momentum, beta,\n",
        "                                   beta1, beta2,\n",
        "                                   epsilon, weight_decay)\n",
        "  cnn.to(device)\n",
        "\n",
        "  epochs = args.epochs\n",
        "  for epochNum in range(epochs):\n",
        "    print(f\"Epoch {epochNum}:\")\n",
        "    for batch_idx, (x_batch, y_batch) in enumerate(train_loader):\n",
        "      if(batch_idx % 40 == 0):\n",
        "        print(f\"Batch idx {batch_idx} running\")\n",
        "        # break\n",
        "      x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
        "      cnn.optimizer.zero_grad()\n",
        "      outputs = cnn(x_batch)\n",
        "      cnn.backward(outputs, y_batch)\n",
        "      cnn.updateWeights()\n",
        "      del x_batch, y_batch, outputs\n",
        "\n",
        "    # Validation accuracy\n",
        "    val_outputs, val_accuracy, val_loss, _, _ = findOutputs(device, cnn, val_loader)\n",
        "    print(f\"validation: loss={val_loss}, accuracy={val_accuracy}\")\n",
        "\n",
        "    # Train accuracy\n",
        "    train_outputs, train_accuracy, train_loss, _, _ = findOutputs(device, cnn, train_loader)\n",
        "    print(f\"training: loss={train_loss}, accuracy={train_accuracy}\")\n",
        "\n",
        "    if val_accuracy > best_val_accuracy:\n",
        "      best_val_accuracy = val_accuracy\n",
        "      best_accuracy_epoch = epochNum\n",
        "\n",
        "    wandb.log({\n",
        "        \"epoch\": epochNum + 1,\n",
        "        \"val_loss\": val_loss,\n",
        "        \"val_accuracy\": val_accuracy,\n",
        "        \"train_loss\": train_loss,\n",
        "        \"train_accuracy\": train_accuracy\n",
        "        },commit=True)\n",
        "    del val_outputs, train_outputs\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "  wandb.log({\n",
        "      \"best_acc_epoch\": best_accuracy_epoch,\n",
        "      \"best_val_accuracy\": best_val_accuracy\n",
        "  })\n",
        "\n",
        "  test_outputs, test_accuracy, test_loss, test_top5_accuracy, test_top2_accuracy = findOutputs(device, cnn, test_loader, True)\n",
        "  print(f\"testing: loss={test_loss}, top1_accuracy={test_accuracy}, top5_accuracy = {test_top5_accuracy}, top2_accuracy = {test_top2_accuracy}\")\n",
        "\n",
        "  wandb.log({\n",
        "      \"test_loss\": test_loss,\n",
        "      \"test_top1_accuracy\": test_accuracy,\n",
        "      \"test_top5_accuracy\": test_top5_accuracy,\n",
        "      \"test_top2_accuracy\": test_top2_accuracy\n",
        "  })\n",
        "  del cnn,train_loader, test_loader, val_loader\n",
        "  gc.collect()\n",
        "  torch.cuda.empty_cache()\n",
        "\n",
        "  wandb.finish()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LxvZjq0eF1tM",
        "outputId": "58a33c54-33ba-429a-c4f2-d7679c92d8b8"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting train_local.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ArgParser"
      ],
      "metadata": {
        "id": "1ZCSkWkJJwnj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile argument_parser.py\n",
        "import argparse\n",
        "\n",
        "def parse_arguments():\n",
        "    parser = argparse.ArgumentParser()\n",
        "\n",
        "    parser.add_argument(\"-wp\", \"--wandb_project\", type=str, default=\"DA6401_Assignment2\",\n",
        "                        help=\"Project name used to track experiments in Weights & Biases dashboard\")\n",
        "    parser.add_argument(\"-we\", \"--wandb_entity\", type=str, default=\"nikhithaa-iit-madras\",\n",
        "                        help=\"Wandb Entity used to track experiments in the Weights & Biases dashboard.\")\n",
        "    parser.add_argument(\"-bd\", \"--base_dir\", type=str, default=\"inaturalist_12K\",\n",
        "                        help=\"Base directory where dataset (train/val folders) are present\")\n",
        "    parser.add_argument(\"-e\", \"--epochs\", type=int, default=10,\n",
        "                        help=\"Number of epochs to train neural network\")\n",
        "    parser.add_argument(\"-b\", \"--batch_size\", type=int, default=32,\n",
        "                        help=\"Batch size used to train neural network\")\n",
        "    parser.add_argument(\"-o\", \"--optimizer\", type=str, choices=[\"sgd\", \"rmsprop\", \"adam\"], default=\"sgd\",\n",
        "                        help=\"Choose one among these optimizers: ['sgd', 'rmsprop', 'adam']\")\n",
        "    parser.add_argument(\"-lr\", \"--learning_rate\", type=float, default=0.001,\n",
        "                        help=\"Learning rate used to optimize model parameters\")\n",
        "    parser.add_argument(\"-m\", \"--momentum\", type=float, default=0.9,\n",
        "                        help=\"Momentum used by momentum and nag optimizers\")\n",
        "    parser.add_argument(\"-beta\", \"--beta\", type=float, default=0.9,\n",
        "                        help=\"Beta used by rmsprop optimizer\")\n",
        "    parser.add_argument(\"-beta1\", \"--beta1\", type=float, default=0.9,\n",
        "                        help=\"Beta1 used by adam and nadam optimizers\")\n",
        "    parser.add_argument(\"-beta2\", \"--beta2\", type=float, default=0.999,\n",
        "                        help=\"Beta2 used by adam and nadam optimizers\")\n",
        "    parser.add_argument(\"-eps\", \"--epsilon\", type=float, default=0.00000001,\n",
        "                        help=\"Epsilon used by optimizers\")\n",
        "    parser.add_argument(\"-w_d\", \"--weight_decay\", type=float, default=0.0001,\n",
        "                        help=\"Weight decay used by optimizers\")\n",
        "    parser.add_argument(\"-dp\", \"--dropout\", type=float, default=0.0,\n",
        "                        help=\"Dropout used in convolution neural network\")\n",
        "    parser.add_argument(\"-da\", \"--isDataAug\", type=str, default=\"False\",\n",
        "                        help=\"Whether to use data augmentation or not\")\n",
        "    parser.add_argument(\"-bn\", \"--isBatchNormalization\", type=str, default=\"False\",\n",
        "                        help=\"Whether to use batch normalization or not\")\n",
        "    parser.add_argument(\"-nf\", \"--num_filters\", type=int, nargs=5,  default=[3, 3, 3, 3, 3],\n",
        "                        help=\"Number of filters used in each convolution layer\")\n",
        "    parser.add_argument(\"-fsz\", \"--filter_sizes\", type=int, nargs=5,  default=[32, 64, 64, 128, 256],\n",
        "                        help=\"Size of filters in each convolution layer\")\n",
        "    parser.add_argument(\"-a\", \"--activation\", type=str, choices=[\"ReLU\", \"SiLU\", \"GELU\"], default=\"SiLU\",\n",
        "                        help=\"Choose one among these activation functions: ['ReLU', 'SiLU', 'GELU']\")\n",
        "    parser.add_argument(\"-ndl\", \"--n_neurons_denseLayer\", type=int, default=128,\n",
        "                        help=\"Number of neurons in dense layer\")\n",
        "\n",
        "    return parser.parse_args()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GbBi5UjJJsXu",
        "outputId": "e667e6c2-4097-48e8-acc8-608cdcce16bc"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting argument_parser.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Main File"
      ],
      "metadata": {
        "id": "ad32BAU8Ny9D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile main.py\n",
        "from train_local import *\n",
        "from argument_parser import *\n",
        "import libraries\n",
        "\n",
        "if __name__==\"__main__\":\n",
        "  args = parse_arguments()\n",
        "  trainNeuralNetwork_local(args)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Ftg2SZXN3Il",
        "outputId": "3a529cf1-c9b6-478a-9e54-4d95dfe62bbd"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting main.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "K4pvuk_CN1hV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Running main"
      ],
      "metadata": {
        "id": "ciWI6M-dONZd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# change epochs\n",
        "!python3 main.py -wp DA6401_Assignment2 -we nikhithaa-iit-madras -b 128 -beta1 0.9 -beta2 0.999 -lr 0.001 -e 1 --base_dir Dataset/inaturalist_12K -o sgd -a SiLU -w_d 0 -nf 3 3 3 3 3 -fsz 32 64 64 128 256 -ndl 128 -dp 0.2 -bn False -da True"
      ],
      "metadata": {
        "id": "qmc4wh3MOQHB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training(Sweep)"
      ],
      "metadata": {
        "id": "2hjZarkeFqPx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile train_sweep.py\n",
        "# import libraries\n",
        "from neural_network import *\n",
        "from data_loader import *\n",
        "from accuracy_calculation import *\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import wandb\n",
        "import gc\n",
        "import os\n",
        "import torch\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)\n",
        "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
        "\n",
        "def trainNeuralNetwork_sweep():\n",
        "  wandb.init(mode=\"online\")\n",
        "  args = wandb.config\n",
        "  train_loader, test_loader, val_loader, num_classes = load_data(args[\"base_dir\"], args[\"isDataAug\"], args[\"batch_size\"])\n",
        "  activationFun = args[\"activation\"]\n",
        "  optimizer = args[\"optimizer\"]\n",
        "  learning_rate = args[\"learning_rate\"]\n",
        "  momentum = args[\"momentum\"]\n",
        "  beta = args[\"beta\"]\n",
        "  beta1 = args[\"beta1\"]\n",
        "  beta2 = args[\"beta2\"]\n",
        "  epsilon = args[\"epsilon\"]\n",
        "  weight_decay = args[\"weight_decay\"]\n",
        "  dropout = args[\"dropout\"]\n",
        "  num_filters = args[\"num_filters\"]\n",
        "  filter_sizes = args[\"filter_sizes\"]\n",
        "  n_neurons_denseLayer = args[\"n_neurons_denseLayer\"]\n",
        "  isBatchNormalization = args[\"isBatchNormalization\"]\n",
        "  isDataAug = args[\"isDataAug\"]\n",
        "\n",
        "  wandb.run.name = f\"{activationFun}_{optimizer}_{dropout}_{n_neurons_denseLayer}_DataAug-{isDataAug}_BatchNorm-{isBatchNormalization}\"\n",
        "  best_val_accuracy = 0.0\n",
        "  best_accuracy_epoch = -1\n",
        "\n",
        "  cnn = ConvolutionalNeuralNetwork(num_classes,\n",
        "                                   num_filters, filter_sizes,\n",
        "                                   activationFun, optimizer,\n",
        "                                   n_neurons_denseLayer,\n",
        "                                   isBatchNormalization, dropout,\n",
        "                                   learning_rate,\n",
        "                                   momentum, beta,\n",
        "                                   beta1, beta2,\n",
        "                                   epsilon, weight_decay)\n",
        "  cnn.to(device)\n",
        "\n",
        "  epochs = args[\"epochs\"]\n",
        "  for epochNum in range(epochs):\n",
        "    print(f\"Epoch {epochNum}:\")\n",
        "    for batch_idx, (x_batch, y_batch) in enumerate(train_loader):\n",
        "      if(batch_idx % 40 == 0):\n",
        "        print(f\"Batch idx {batch_idx} running\")\n",
        "      x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
        "      cnn.optimizer.zero_grad()\n",
        "      outputs = cnn(x_batch)\n",
        "      cnn.backward(outputs, y_batch)\n",
        "      cnn.updateWeights()\n",
        "      del x_batch, y_batch, outputs\n",
        "\n",
        "    # Validation accuracy\n",
        "    val_outputs, val_accuracy, val_loss, _, _ = findOutputs(device, cnn, val_loader)\n",
        "    # wandb.run.summary[\"metric_name\"] = val_accuracy\n",
        "    print(f\"validation: loss={val_loss}, accuracy={val_accuracy}\")\n",
        "\n",
        "    # Train accuracy\n",
        "    train_outputs, train_accuracy, train_loss, _, _ = findOutputs(device, cnn, train_loader)\n",
        "    print(f\"training: loss={train_loss}, accuracy={train_accuracy}\")\n",
        "\n",
        "    if val_accuracy > best_val_accuracy:\n",
        "      best_val_accuracy = val_accuracy\n",
        "      best_accuracy_epoch = epochNum\n",
        "\n",
        "    wandb.log({\n",
        "        \"epoch\": epochNum + 1,\n",
        "        \"val_loss\": val_loss,\n",
        "        \"val_accuracy\": val_accuracy,\n",
        "        \"train_loss\": train_loss,\n",
        "        \"train_accuracy\": train_accuracy\n",
        "        },commit=True)\n",
        "    del val_outputs, train_outputs\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "  wandb.log({\n",
        "      \"best_acc_epoch\": best_accuracy_epoch,\n",
        "      \"best_val_accuracy\": best_val_accuracy\n",
        "  })\n",
        "\n",
        "  test_outputs, test_accuracy, test_loss, test_top5_accuracy, test_top2_accuracy = findOutputs(device, cnn, test_loader, True)\n",
        "  print(f\"testing: loss={test_loss}, top1_accuracy={test_accuracy}, top5_accuracy = {test_top5_accuracy}, top2_accuracy = {test_top2_accuracy}\")\n",
        "\n",
        "  wandb.log({\n",
        "      \"test_loss\": test_loss,\n",
        "      \"test_top1_accuracy\": test_accuracy,\n",
        "      \"test_top5_accuracy\": test_top5_accuracy,\n",
        "      \"test_top2_accuracy\": test_top2_accuracy\n",
        "  })\n",
        "  del cnn,train_loader, test_loader, val_loader\n",
        "  gc.collect()\n",
        "  torch.cuda.empty_cache()\n",
        "\n",
        "  wandb.finish()"
      ],
      "metadata": {
        "id": "zEGF79uTtolO",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-19T12:34:03.227474Z",
          "iopub.execute_input": "2025-04-19T12:34:03.227764Z",
          "iopub.status.idle": "2025-04-19T12:34:03.243571Z",
          "shell.execute_reply.started": "2025-04-19T12:34:03.227745Z",
          "shell.execute_reply": "2025-04-19T12:34:03.242831Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d6bca90c-ae2a-4812-dc0d-fdfd9e39ea69"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting train_sweep.py\n"
          ]
        }
      ],
      "execution_count": 30
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Main Sweep File"
      ],
      "metadata": {
        "id": "Yk10zqi9OEpq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile main_sweep.py\n",
        "from train_sweep import *\n",
        "# import libraries\n",
        "import wandb\n",
        "\n",
        "# best_acc_sweep_configuration = {\n",
        "#     \"method\": \"random\",\n",
        "#     \"name\" : \"test_sweep1\",\n",
        "#     \"parameters\": {\n",
        "#         \"num_filters\": {'values': [[256, 128, 64, 64, 32]]},\n",
        "#         \"filter_sizes\": {'values': [[3, 3, 3, 3, 3]]},\n",
        "#         \"activation\": {\"values\": [\"SiLU\"]},\n",
        "#         \"optimizer\": {\"values\": [\"sgd\"]},\n",
        "#         \"learning_rate\": {\"values\": [1e-3]},\n",
        "#         \"weight_decay\": {\"values\": [0.0001]},\n",
        "#         \"momentum\": {\"values\": [0.9]},\n",
        "#         \"beta\": {\"values\": [0.9]},\n",
        "#         \"beta1\": {\"values\":[0.9]},\n",
        "#         \"beta2\": {\"values\": [0.999]},\n",
        "#         \"epsilon\": {\"values\": [1e-8]},\n",
        "#         # \"base_dir\": {\"values\":[\"/content/drive/MyDrive/DL_Assignment2/Dataset/inaturalist_12K/\"]},\n",
        "#         \"base_dir\": {\"values\": [\"/kaggle/input/inaturalist/inaturalist_12K\"]},\n",
        "#         \"isDataAug\": {\"values\": [\"False\"]},\n",
        "#         \"isBatchNormalization\": {\"values\": [\"False\"]},\n",
        "#         \"dropout\": {\"values\": [0.3]},\n",
        "#         \"n_neurons_denseLayer\": {\"values\": [128]},\n",
        "#         \"batch_size\": {\"values\": [32]},\n",
        "#         \"epochs\": {\"values\": [10]}\n",
        "#     }\n",
        "# }\n",
        "\n",
        "sweep_configuration = {\n",
        "    \"method\": \"random\",\n",
        "    \"name\" : \"train_sweep_final2_v1\",\n",
        "    \"parameters\": {\n",
        "        \"num_filters\": {'values': [[32, 32, 32, 32, 32], [32, 64, 64, 128, 256], [256, 128, 64, 64, 32]]},\n",
        "        \"filter_sizes\": {'values': [[3, 3, 3, 3, 3], [5, 5, 5, 5, 5],[3,3,5,5,1]]},\n",
        "        \"activation\": {\"values\": [\"ReLU\", \"SiLU\", \"GELU\"]},\n",
        "        \"optimizer\": {\"values\": [\"adam\", \"rmsprop\", \"sgd\"]},\n",
        "        \"learning_rate\": {\"values\": [1e-3]},\n",
        "        \"weight_decay\": {\"values\": [0.0001]},\n",
        "        \"momentum\": {\"values\": [0.9]},\n",
        "        \"beta\": {\"values\": [0.9]},\n",
        "        \"beta1\": {\"values\":[0.9]},\n",
        "        \"beta2\": {\"values\": [0.999]},\n",
        "        \"epsilon\": {\"values\": [1e-8]},\n",
        "        \"base_dir\": {\"values\":[\"/content/drive/MyDrive/DL_Assignment2/Dataset/inaturalist_12K/\"]},\n",
        "        # \"base_dir\": {\"values\": [\"/kaggle/input/inaturalist/inaturalist_12K\"]},\n",
        "        \"isDataAug\": {\"values\": [\"False\", \"True\"]},\n",
        "        \"isBatchNormalization\": {\"values\": [\"True\", \"False\"]},\n",
        "        \"dropout\": {\"values\": [0.2, 0.3]},\n",
        "        \"n_neurons_denseLayer\": {\"values\": [128, 256]},\n",
        "        \"batch_size\": {\"values\": [32,64]},\n",
        "        \"epochs\": {\"values\": [5,10]}\n",
        "    }\n",
        "}\n",
        "\n",
        "if __name__==\"__main__\":\n",
        "  wandb.login()\n",
        "  wandb_id = wandb.sweep(sweep_configuration, project=\"DA6401_Assignment2\")\n",
        "  wandb.agent(wandb_id, function=trainNeuralNetwork_sweep)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NjqVLPNJI1Wy",
        "outputId": "cda85b4d-e647-4a15-d93c-a9c05009e14d"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting main_sweep.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Running"
      ],
      "metadata": {
        "id": "kok522ZR5NWr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 main_sweep.py"
      ],
      "metadata": {
        "id": "ou4N_DlH5MxQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 10 x 3 Grid Visualization (Havent written into file)\n",
        "\n",
        "> Add blockquote\n",
        "\n"
      ],
      "metadata": {
        "id": "biLBAhIRCanR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "best_acc_sweep_configuration1 = {\n",
        "    \"method\": \"random\",\n",
        "    \"name\" : \"visualize_sweep_final\",\n",
        "    \"parameters\": {\n",
        "        \"num_filters\": {'values': [[256, 128, 64, 64, 32]]},\n",
        "        \"filter_sizes\": {'values': [[3, 3, 3, 3, 3]]},\n",
        "        \"activation\": {\"values\": [\"SiLU\"]},\n",
        "        \"optimizer\": {\"values\": [\"sgd\"]},\n",
        "        \"learning_rate\": {\"values\": [1e-3]},\n",
        "        \"weight_decay\": {\"values\": [0.0001]},\n",
        "        \"momentum\": {\"values\": [0.9]},\n",
        "        \"beta\": {\"values\": [0.9]},\n",
        "        \"beta1\": {\"values\":[0.9]},\n",
        "        \"beta2\": {\"values\": [0.999]},\n",
        "        \"epsilon\": {\"values\": [1e-8]},\n",
        "        # \"base_dir\": {\"values\":[\"/content/drive/MyDrive/DL_Assignment2/Dataset/inaturalist_12K/\"]},\n",
        "        \"base_dir\": {\"values\": [\"/kaggle/input/inaturalist/inaturalist_12K\"]},\n",
        "        \"isDataAug\": {\"values\": [\"False\"]},\n",
        "        \"isBatchNormalization\": {\"values\": [\"False\"]},\n",
        "        \"dropout\": {\"values\": [0.3]},\n",
        "        \"n_neurons_denseLayer\": {\"values\": [128]},\n",
        "        \"batch_size\": {\"values\": [32]},\n",
        "        \"epochs\": {\"values\": [10]}\n",
        "    }\n",
        "}\n",
        "\n",
        "def unnormalize(img):\n",
        "    mean = [0.485, 0.456, 0.406]\n",
        "    std = [0.229, 0.224, 0.225]\n",
        "    for t, m, s in zip(img, mean, std):\n",
        "        t.mul_(s).add_(m)\n",
        "    return torch.clamp(img, 0, 1)\n",
        "\n",
        "def visualizeOutputs(cnn, testDataLoader):\n",
        "  cnn.eval()  # setting the model to evaluation model\n",
        "  total_loss = 0.0\n",
        "  n_correct = 0\n",
        "  n_correct_top2 = 0\n",
        "  n_samples = 0\n",
        "\n",
        "  pred_output_perLabel = [[] for i in range(10)]\n",
        "  x_values_perLabel = [[] for i in range(10)]\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for batch_idx, (x_batch, y_batch) in enumerate(testDataLoader):\n",
        "      x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
        "      batch_outputs = cnn(x_batch)\n",
        "\n",
        "      loss = nn.CrossEntropyLoss()(batch_outputs, y_batch)\n",
        "      total_loss += loss.item() * x_batch.size(0)\n",
        "\n",
        "      y_pred_batch = torch.argmax(batch_outputs, dim=1)\n",
        "      n_correct += (y_pred_batch == y_batch).sum().item()\n",
        "      n_samples += x_batch.size(0)\n",
        "\n",
        "      y_pred_batch_top2 = torch.topk(batch_outputs, 2, dim=1).indices\n",
        "      n_correct_top2 += y_pred_batch_top2.eq(y_batch.view(-1, 1)).sum().item()\n",
        "\n",
        "      # Collecting 3 images per class\n",
        "      for i in range(len(y_batch)):\n",
        "        label = y_batch[i].item()\n",
        "        if len(pred_output_perLabel[label]) < 3:\n",
        "           x_values_perLabel[label].append(x_batch[i].cpu())\n",
        "           pred_output_perLabel[label].append(y_pred_batch[i].item())\n",
        "\n",
        "  labelNameList = [\"Amphibia\", \"Animalia\", \"Arachnida\", \"Aves\", \"Fungi\", \"Insecta\", \"Mammalia\", \"Mollusca\", \"Plantae\", \"Reptilia\"]\n",
        "  # Visualizing collected results\n",
        "  fig, axes = plt.subplots(10, 3, figsize=(10, 20))\n",
        "  plt.title(\"True Vs Pred results for each class label\")\n",
        "  for i in range(10):\n",
        "     for j in range(3):\n",
        "       predLabel = pred_output_perLabel[i][j]\n",
        "       trueLabel = i\n",
        "       print(f\"True = {trueLabel}, Pred = {predLabel}\")\n",
        "       image = x_values_perLabel[i][j]\n",
        "       image = unnormalize(image)\n",
        "       ax = axes[i, j]\n",
        "       ax.imshow(image.permute(1, 2, 0))\n",
        "       ax.set_title(f'TrueLabel: {labelNameList[trueLabel]}\\nPredLabel: {labelNameList[predLabel]}')\n",
        "       ax.axis('off')\n",
        "  plt.tight_layout()\n",
        "  wandb.log({\"Grid\": wandb.Image(plt)})\n",
        "  plt.show()\n",
        "\n",
        "  accuracy = (n_correct * 100.0) / n_samples\n",
        "  avg_loss = total_loss / n_samples\n",
        "  top2_accuracy = (n_correct_top2 * 100.0) / n_samples\n",
        "  return accuracy, avg_loss, top2_accuracy\n",
        "\n",
        "def visualization_sweep():\n",
        "  wandb.init(mode=\"online\")\n",
        "  args = wandb.config\n",
        "  train_loader, test_loader, val_loader, num_classes = load_data(args[\"base_dir\"], args[\"isDataAug\"], args[\"batch_size\"])\n",
        "  activationFun = args[\"activation\"]\n",
        "  optimizer = args[\"optimizer\"]\n",
        "  learning_rate = args[\"learning_rate\"]\n",
        "  momentum = args[\"momentum\"]\n",
        "  beta = args[\"beta\"]\n",
        "  beta1 = args[\"beta1\"]\n",
        "  beta2 = args[\"beta2\"]\n",
        "  epsilon = args[\"epsilon\"]\n",
        "  weight_decay = args[\"weight_decay\"]\n",
        "  dropout = args[\"dropout\"]\n",
        "  num_filters = args[\"num_filters\"]\n",
        "  filter_sizes = args[\"filter_sizes\"]\n",
        "  n_neurons_denseLayer = args[\"n_neurons_denseLayer\"]\n",
        "  isBatchNormalization = args[\"isBatchNormalization\"]\n",
        "  isDataAug = args[\"isDataAug\"]\n",
        "\n",
        "  wandb.run.name = f\"{activationFun}_{optimizer}_{dropout}_{n_neurons_denseLayer}_DataAug-{isDataAug}_BatchNorm-{isBatchNormalization}\"\n",
        "  best_val_accuracy = 0.0\n",
        "  best_accuracy_epoch = -1\n",
        "\n",
        "  cnn = ConvolutionalNeuralNetwork(num_classes,\n",
        "                                   num_filters, filter_sizes,\n",
        "                                   activationFun, optimizer,\n",
        "                                   n_neurons_denseLayer,\n",
        "                                   isBatchNormalization, dropout,\n",
        "                                   learning_rate,\n",
        "                                   momentum, beta,\n",
        "                                   beta1, beta2,\n",
        "                                   epsilon, weight_decay)\n",
        "  cnn.to(device)\n",
        "\n",
        "  epochs = args[\"epochs\"]\n",
        "  for epochNum in range(epochs):\n",
        "    print(f\"Epoch {epochNum}:\")\n",
        "    for batch_idx, (x_batch, y_batch) in enumerate(train_loader):\n",
        "      if(batch_idx % 40 == 0):\n",
        "        print(f\"Batch idx {batch_idx} running\")\n",
        "      x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
        "      cnn.optimizer.zero_grad()\n",
        "      outputs = cnn(x_batch)\n",
        "      cnn.backward(outputs, y_batch)\n",
        "      cnn.updateWeights()\n",
        "      del x_batch, y_batch, outputs\n",
        "\n",
        "  test_accuracy, test_loss, test_top2_accuracy = visualizeOutputs(cnn, test_loader)\n",
        "  print(f\"testing: loss={test_loss}, top1_accuracy={test_accuracy}, top2_accuracy = {test_top2_accuracy}\")\n",
        "\n",
        "  wandb.log({\n",
        "      \"test_loss\": test_loss,\n",
        "      \"test_top1_accuracy\": test_accuracy,\n",
        "      \"test_top2_accuracy\": test_top2_accuracy\n",
        "  })\n",
        "\n",
        "\n",
        "  del cnn,train_loader, test_loader, val_loader\n",
        "  gc.collect()\n",
        "  torch.cuda.empty_cache()\n",
        "\n",
        "  wandb.finish()\n",
        "\n",
        "wandb.login(key=\"x\")\n",
        "wandb_id = wandb.sweep(best_acc_sweep_configuration1, project=\"DA6401_Assignment2\")\n",
        "wandb.agent(wandb_id, function=visualization_sweep)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-19T16:34:18.503675Z",
          "iopub.execute_input": "2025-04-19T16:34:18.504242Z",
          "iopub.status.idle": "2025-04-19T16:47:59.080757Z",
          "shell.execute_reply.started": "2025-04-19T16:34:18.504210Z",
          "shell.execute_reply": "2025-04-19T16:47:59.079578Z"
        },
        "id": "GRzsyAkACanR"
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}